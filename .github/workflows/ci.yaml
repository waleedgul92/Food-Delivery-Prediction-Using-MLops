name: Python Application CI

on:
  push:
    branches: [ main, develop, master ]
  pull_request:
    branches: [ main ]

env:
  PYTHON_VERSION: "3.11"

jobs:
  build-and-train:
    runs-on: windows-latest
    
    steps:
    # 1. Checkout repository
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    # 2. Set up Python
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip' # This correctly handles pip caching

    # 3. Install Python dependencies
    - name: Install Python dependencies
      shell: pwsh # Use PowerShell, the default for windows-latest
      run: |
        python -m pip install --upgrade pip
        # Use PowerShell syntax for checking if file exists
        if (Test-Path -Path requirements.txt) {
          pip install -r requirements.txt
        } else {
          Write-Host "requirements.txt not found, skipping."
        }

    # 4. Install DVC with S3 support
    - name: Install DVC
      shell: pwsh
      run: |
        pip install --upgrade "dvc[s3]"

    # 5. Configure AWS credentials for S3 access
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.WS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_DEFAULT_REGION }}
        # Credentials are now available as env vars for subsequent steps

    # 6. Pull other DVC-tracked data (if any)
    - name: Pull DVC data
      shell: pwsh
      run: |
        dvc pull
        # This will skip raw_dataset (since it's not a dep)
        # but will pull any other models/data

    # 7. Run DVC pipeline
    - name: Run DVC pipeline
      shell: pwsh
      env:
        DAGSHUB_PAT: ${{ secrets.DAGSHUB_KEY }}
        # AWS keys are already set in the environment from Step 5
      run: |
        dvc repro